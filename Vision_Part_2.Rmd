---
title: "Vision Project Part 2"
author: "Brian Dang & Ethan Mai & Naomi Wilcox"
date: "11/22/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F)
library(nnet)
library(MASS)
library(ggplot2)
library(GGally)
library(kableExtra)
library(dplyr)
library(glmnet)
library(class)
library(knitr)

data = read.csv('~/Downloads/dataframev2.csv')
data <- data %>%
  rename(
    "irf3" = "vol_irf3",
    "rpe3" = "vol_rpe3",
    "srf3" = "vol_srf3",
    "ped3" = "vol_ped3",
    "shrm3" = "vol_shrm3",
    "irhr3" = "vol_intrarethyperreflect3"
  )

#Create new multiclass outcome variable
data$class <- cut(data$va5, breaks = c(100,85,70,60,35,0), labels = c(1,2,3,4,5))
table(data$class)
#1 = no visual impairement (VI), 2= mild VI, 3=moderate VI, 4=blind, 5=severe blindness
```

Table of Contents:

1. kNN: Normalize variables via z-score

2. Create training and testing sets

3. kNN: Test Initial Models

4. kNN: Determine Optimal k

5. kNN: Final Model

6. Multinomial Regression

7. Pairs plots

8. Lasso

9. Finding optimal lambda

10. Lasso Prediction

# 1. kNN: Normalize variables via z-score

```{r}
summary(data$srf3)
summary(data$shrm3) #these two are similar in mean

summary(data$irf3) #very small mean

summary(data$ped3) #both large in mean
summary(data$rpe3) #though RPE has a small maximum value
```

We conduct kNN with the Euclidean distance metric. As suggested by the above, the effect in kNN of one biomarker is generally not equal to those of the others. For instance, IRF biomarker is in a significantly smaller numeric scale than other variables. Normalization improves this to generally give equal weight to each predictor. Here we standardize by the z-score: http://cs.wellesley.edu/~cs305/lectures/3_kNN.pdf 

Trade-off here: the Euclidean distances in the kNN used are no longer indicative of the volume. Despite this we care more about each feature having equal weight.

```{r}
#Gender made binary for kNN
data$gender.n=as.numeric(data$gender)
for (i in 1:length(data$gender)){
  if (data$gender[i] == "Male") {
    data$gender.n[i] = -1
  } else {
    data$gender.n[i] = 1
  }
}

##Ethnicity for kNN
data$eth.afro=as.numeric(data$ethnicity) #initialize
for (i in 1:length(data$ethnicity)){
  if (data$ethnicity[i] == "afrocaribbean") {
    data$eth.afro[i] = 1
  } else {
    data$eth.afro[i] = -1
  }
}

data$eth.asian=as.numeric(data$ethnicity) #initialize
for (i in 1:length(data$ethnicity)){
  if (data$ethnicity[i] == "asian") {
    data$eth.asian[i] = 1
  } else {
    data$eth.asian[i] = -1
  }
}

data$eth.cau=as.numeric(data$ethnicity) #initialize
for (i in 1:length(data$ethnicity)){
  if (data$ethnicity[i] == "caucasian") {
    data$eth.cau[i] = 1
  } else {
    data$eth.cau[i] = -1
  }
}

data$eth.other=as.numeric(data$ethnicity) #initialize
for (i in 1:length(data$ethnicity)){
  if (data$ethnicity[i] == "Other") {
    data$eth.other[i] = 1
  } else {
    data$eth.other[i] = -1
  }
}

data$eth.unknown=as.numeric(data$ethnicity) #initialize
for (i in 1:length(data$ethnicity)){
  if (data$ethnicity[i] == "unknown") {
    data$eth.unknown[i] = 1
  } else {
    data$eth.unknown[i] = -1
  }
}

#levels(as.factor(data$agegroup))
levels(as.factor(data$ethnicity))
```

```{r}
#standardization of biomarkers for kNN
colnames(data[,c(13, 18, 23, 28, 33, 8)])
indx=c(13, 18, 23, 28, 33, 8)
for (i in 1:6){
  data[,indx[i]]=(data[,indx[i]]-mean(data[,indx[i]]))/sd(data[,indx[i]]) #standardize using z-score
}

#summary(data[,13]) #double check
#summary(data[,18])
```

# 2. Create training and testing sets

```{r}
nn <- dim(data)[1]
set.seed(123)
tst <- sample(1:nn, ceiling(nn/3), replace = F)
test <- data[tst,]
train <- data[-tst,]

#Comparing the two sets
hist(test$va5)
hist(train$va5)
table(test$class)
table(train$class)
```

# 3. kNN: Test Initial Models

The non-numerical features used here are: 'agegroup', 'gender', and 'ethnicity'. We treat these as follows:

-Age group is ordinal. There does not seem to be an easy way to conform this to the Euclidean distance metric, so we omit this variable.
-Gender is binary, coded as -1 for male, 1 for female. This coding is done to make it similar to the z-score normalized data used in the other predictors.
-Ethnicity is made into a binary variable, in the same manner as gender, for each measured ethnicity. Therefore there are 5 binary dummy variables to encompass the following ethnicity categories in the data: Afro-Caribbean, Asian, Caucasian, Other, and Unknown.

We attempt the following combinations of predictors: only biomarkers+VA, all predictors, biomarkers+VA+gender, and biomarkers+VA+ethnicity.

```{r}
ks=c(5, 9, 11, 13, 15, 21, 31) # choices of k that we will try
misclass.knn=as.data.frame(cbind(ks, ks, ks, ks)) #initialize
misclass.knncv=as.data.frame(cbind(ks, ks)) #initialize

rownames(misclass.knn)=c("k=5", "9", "11", "13", "15", "21", "31")
colnames(misclass.knn)=c("biomarker+VA misclassification", "full kNN", "
                         biomarkers+VA+gender", "biomarkers+VA+ethnicity")

xnames.bio <- c("ped3", "shrm3", "irf3", "srf3", "rpe3", "va3")
xnames <- c("ped3", "shrm3", "irf3", "srf3", "rpe3", "gender.n", "eth.afro", 
            "eth.asian", "eth.cau", "eth.other", "eth.unknown", "va3")
xnames.gen <- c("ped3", "shrm3", "irf3", "srf3", "rpe3", "gender.n", "va3")
xnames.eth <- c("ped3", "shrm3", "irf3", "srf3", "rpe3", "eth.afro", 
            "eth.asian", "eth.cau", "eth.other", "eth.unknown", "va3")

for (i in 1:length(ks)){
  knn.pred.bio <- knn(train[,xnames.bio], test[,xnames.bio],train$class, k=ks[i]) #biomarker+VA-only model
  knn.pred <- knn(train[,xnames], test[,xnames],train$class, k=ks[i]) 
  #model with all predictors
  knn.pred.gen <- knn(train[,xnames.gen], test[,xnames.gen],train$class, k=ks[i]) 
  knn.pred.eth <- knn(train[,xnames.eth], test[,xnames.eth],train$class, k=ks[i]) 

  tbl.bio <- table(knn.pred.bio, test$class) #tables of predicted vs true class.
  tbl <- table(knn.pred, test$class) #diagonals are agreement b/t model and truth
  tbl.gen <- table(knn.pred.gen, test$class)
  tbl.eth <- table(knn.pred.eth, test$class)
  
  # (number of misclassifications)/total
  mis.bio=(nrow(test)-sum(diag(tbl.bio)))/nrow(test) # biomarker-only
  mis=(nrow(test)-sum(diag(tbl)))/nrow(test) #misclassification from full kNN
  mis.gen=(nrow(test)-sum(diag(tbl.gen)))/nrow(test) 
  mis.eth=(nrow(test)-sum(diag(tbl.eth)))/nrow(test)
  
  misclass.knn[i,1]=mis.bio #store values
  misclass.knn[i,2]=mis
  misclass.knn[i,3]=mis.gen
  misclass.knn[i,4]=mis.eth
}
##k=5

misclass.knn
```


From the above we notice that the biomarker+VA misclassification is consistently the lowest, and thus the best. Thus, we continue with only the biomarkers+VA at time 3 as features.

# 4. kNN: Determine Optimal k

Below is repeated LOOCV with the entire dataset, not with the previously specified training set. LOOCV is implemented only for the purpose of finding the optimal k here.

```{r}
##repeated LOOCV
library(boot)
numcv=200 #number of repetitions
cv.rep.mis <- as.data.frame(matrix(0, nrow=numcv, ncol=length(ks))) #dataframe of misclassification errors
colnames(cv.rep.mis)=rownames(misclass.knn)

for (z in 1:numcv){
  for (i in 1:length(ks)){
    knncv.pred.bio <- knn.cv(data[,xnames.bio], data$class, k=ks[i])
    tbl.bio <- table(knncv.pred.bio, data$class)
    mis.bio=(nrow(data)-sum(diag(tbl.bio)))/nrow(data) # biomarker-only
    cv.rep.mis[z, i]=mis.bio #store values
  }
}
head(cv.rep.mis)
boxplot(cv.rep.mis, ylab='Misclassification rate', xlab='k in kNN', 
        title='LOOCV over 200 repetitions')
```

# 5. kNN: Final Model

It seems that k=15 is optimal. Thus this is the k for our final model. Below we find the error rate over 2000 repetitions of this final model, given the original test-train data split.

```{r}
mis.final=numeric(2000) # vector of misclassification error 

for (z in 1:2000){
    knn.final <- knn(train[,xnames.bio], test[,xnames.bio],train$class, k=15) #biomarker+VA-only model
    tbl.bio <- table(knn.final, test$class)
    mis.bio=(nrow(test)-sum(diag(tbl.bio)))/nrow(test)
    mis.final[z]=mis.bio #store values
  }

summary(mis.final)

```

# 6. Multinomial Regression

```{r}
data = read.csv('~/Downloads/dataframev2.csv')
data <- data %>%
  rename(
    "irf4" = "vol_irf4",
    "rpe4" = "vol_rpe4",
    "srf4" = "vol_srf4",
    "ped4" = "vol_ped4",
    "shrm4" = "vol_shrm4",
    "irhr4" = "vol_intrarethyperreflect4",
    "irf3" = "vol_irf3",
    "rpe3" = "vol_rpe3",
    "srf3" = "vol_srf3",
    "ped3" = "vol_ped3",
    "shrm3" = "vol_shrm3",
    "irhr3" = "vol_intrarethyperreflect3"
  ) 


data$agegroup <- factor(data$agegroup)
data$ethnicity <- factor(data$ethnicity)
data$gender <- factor(data$gender)
levels(data$gender) <- c(1,0) #female = 1
levels(data$agegroup) <- c(1,2,3,4) #1= 50-59,2= 60-69,3= 70-79,4= 80+
levels(data$ethnicity) <- c(1,2,3,4,5) #1=afrocaribbean, 2=asian, 3=caucasian, 4=other, 5=unkown

#Create new multiclass outcome variable
data$class <- cut(data$va5, breaks = c(100,85,70,60,35,0), labels = c(1,2,3,4,5))
data$class3 <- cut(data$va3, breaks = c(100,85,70,60,35,0), labels = c(1,2,3,4,5), include.lowest = T)
table(data$class)
#1 = no visual impairement (VI), 2= mild VI, 3=moderate VI, 4=blind, 5=severe blindness


#create change in va1 and va3 variable
data$change <- cut(data$va3-data$va1, breaks = c(-Inf,0,Inf), labels = c("Decrease","Increase"))
data$blind <- ifelse(data$va5 < 60,1,0)
data$improve <- ifelse(data$va5 >= data$va3,1,0) #or time 1


#Create training and testing sets
nn <- dim(data)[1]
set.seed(123)
tst <- sample(1:nn, ceiling(nn/5), replace = F)
test <- data[tst,]
train <- data[-tst,]
```

# 7. Pairs plots

```{r}
ggpairs(data, columns = c(42, 8,5,3, 13,18,23,28,33,38), columnLabels = c("Class","VA3","Gender","Ethnicity", "IRF3","RPE3","SRF3","PED3","SHRM3","IRHR3"))
```

```{r}
#"changeirf","changerpe","changesrf","changeped","changeshrm","changeirhr"
xnams <- c("va3", "irf3", "rpe3", "srf3", "ped3", "shrm3","irhr3")
fmla2 <- as.formula(paste("class ~ ", paste(xnams, collapse= "+")))


mlogist <- multinom(fmla2, data=train, na.actoin=na.omit)
smlogist <- summary(mlogist)
lab <- c("Incercept", "VA3","IRF3","RPE3","SRF3","PED3","SHRM3","IRHR3")
kable(smlogist$coefficients, caption = "Coefficients for multinomial regression", col.names = lab) %>%
  kable_styling(latex_options = c("striped")) %>%
  landscape

kable(smlogist$standard.errors, caption = "Standard errors for model coefficients", col.names = lab) %>%
  kable_styling(latex_options = c("striped")) %>%
  landscape
#exp(coef(mlogist))

##inference
z <- summary(mlogist)$coeff/summary(mlogist)$standard.error
p <- (1-pnorm(abs(z),0,1))*2
kable(p, caption = "2-tailed z test for model coefficients", col.names = lab) %>%
  kable_styling(latex_options = c("striped")) %>%
  landscape

kable(cbind(1:5,table(Predicted=predict(mlogist, newdata = test),True=test$class)), row.names = F, caption = "Test Set: Predicted vs. True classification", col.names = c("Predicted Class","1","2","3","4","5")) %>%
  add_header_above(header = c(" ","True Class"=5))

kable(cbind(1:5,table(Predicted=predict(mlogist, newdata = train),True=train$class)), row.names = F, caption = "Training Set: Predicted vs. True classification", col.names = c("Predicted Class","1","2","3","4","5")) %>%
  add_header_above(header = c(" ","True Class"=5))

##accuracy
sum(diag(table(predict(mlogist,newdata = test),test$class)))/sum(table(predict(mlogist, newdata = test),test$class))
sum(diag(table(predict(mlogist,newdata = train),train$class)))/sum(table(predict(mlogist, newdata = train),train$class))
```



$$
\begin{aligned}
p_I=P(Y=I| \vec X)= \frac{ exp \left( \beta_0 + \beta_{1I}VA3 + \beta_{2I}IRF3 + \beta_{3I}RPE3 + ... +\beta_{8I}IRHR3 \right)}{1+\sum_{k=1}^4 exp \left( \beta_0 + \beta_{1k}VA3 + \beta_{2k}IRF3 + \beta_{3k}RPE3 + ... +\beta_{8k}IRHR3 \right)} \\
p_1 = P(Y=1|\vec X) = \frac{1}{1+\sum_{k=1}^4 exp \left( \beta_0 + \beta_{1k}VA3 + \beta_{2k}IRF3 + \beta_{3k}RPE3 + ... +\beta_{8k}IRHR3\right)}
\end{aligned}
$$

# 8. Lasso
```{r}
xxx <- model.matrix(~.,train[,xnams])
yyy <- train$class
gridd <- c(exp(seq(2,-8,-0.5)),0)
lasso2 <- glmnet(xxx,yyy, family = "multinomial", alpha = 1, lambda = gridd)
lasso2$beta
```

# 9. Finding optimal lambda
```{r}
set.seed(123)
cv.lso2 <- cv.glmnet(xxx,yyy,family="multinomial",type.multinomial = "grouped", alpha = 1, intercept = T)
(lmin<- cv.lso2$lambda.min)
log(cv.lso2$lambda.min)
index <- which(cv.lso2$lambda == lmin)
plot(cv.lso2, main="Lasso")
l <- cv.lso2$glmnet.fit$beta

l2 <- rbind(l$'1'[,index], l$'2'[,index], l$'3'[,index], l$'4'[,index], l$'5'[,index])
kable(cbind(1:5,l2), col.names = c("Class",lab), caption = "Training Set: LASSO Coefficients") %>%
  kable_styling(latex_options = c("striped")) %>%
  landscape
```


# 10. Lasso Prediction
```{r}
lasso3 <- glmnet(xxx,yyy, family = "multinomial", alpha = 1)
lso.train.pred <- predict(lasso3, newx =  model.matrix(~.,train[,xnams]), s=lmin, type = "class")
lso.test.pred <- predict(lasso3, newx =  model.matrix(~.,test[,xnams]), s=lmin, type = "class")

t1 <- table(Predicted = lso.train.pred,True = train$class)
t2 <- table(Predicted = lso.test.pred, True = test$class)

kable(cbind(1:4,t2), caption = "Test Set: Predicted vs. True classification", col.names = c("Predicted Class","1","2","3","4","5")) %>%
  add_header_above(header = c(" ","True Class"=5))

kable(cbind(1:4,t1), caption = "Training Set: Predicted vs. True classification", col.names = c("Predicted Class","1","2","3","4","5")) %>%
  add_header_above(header = c(" ","True Class"=5))




##accuracy
sum(diag(t1))/sum(t1)
sum(diag(t2))/sum(t2)
```



